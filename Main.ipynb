{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRSq6sSTqZYK",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrSDf_h0B9C0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Download and install env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SfFgW911B09t",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Colab installs\n",
    "#!pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "#!pip install tqdm\n",
    "#!pip install torchvision\n",
    "#!pip install seaborn\n",
    "#!pip install sklearn\n",
    "#!pip install numpy\n",
    "#!pip install opencv-python\n",
    "#!pip install onnx onnx2pytorch\n",
    "\n",
    "# ## commands to install the env\n",
    "!git clone https://github.com/MultiAgentLearning/playground ./pommer_setup\n",
    "!pip install -U ./pommer_setup\n",
    "!rm -rf ./pommer_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6iIve1wBVfcz",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/RLCommunity/graphic_pomme_env ./graphic_pomme_env\n",
    "!pip install -U ./graphic_pomme_env\n",
    "!rm -rf ./graphic_pomme_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFQNjVpzB4NC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BWnfGIBWqpqu",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.onnx\n",
    "import shutil\n",
    "from time import strftime, time\n",
    "from collections import deque, namedtuple\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display\n",
    "from functools import partial\n",
    "from itertools import chain\n",
    "\n",
    "from torch.distributions import Categorical\n",
    "!pip install onnx\n",
    "import onnx\n",
    "!pip install onnx2pytorch\n",
    "from onnx2pytorch import ConvertModel\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import gym\n",
    "from gym import Env, Wrapper\n",
    "from gym import logger as gymlogger\n",
    "# Environment import and set logger level to display error only\n",
    "gymlogger.set_level(40)  # error only\n",
    "\n",
    "import gym\n",
    "from gym import Env, Wrapper\n",
    "\n",
    "from pommerman import make\n",
    "from pommerman.agents import BaseAgent, RandomAgent, SimpleAgent\n",
    "from graphic_pomme_env import graphic_pomme_env\n",
    "from graphic_pomme_env.wrappers import PommerEnvWrapperFrameSkip2\n",
    "\n",
    "print('''Hint: just ignore the error \"Import error NSDE! You will not be able to render --> Cannot connect to 'None'\"''')\n",
    "pomenvs = [es.id for es in gym.envs.registry.all() if es.id.startswith('Pomme')]\n",
    "print(\"\\n\".join(pomenvs))\n",
    "res = graphic_pomme_env.load_resources()\n",
    "N_PLAYERS = 2 \n",
    "NUM_STACK = 5\n",
    "NUM_ACTIONS = 6\n",
    "'''\n",
    "0 Stop\n",
    "1 Up\n",
    "2 Down\n",
    "3 Left\n",
    "4 Right\n",
    "5 Bomb\n",
    "'''\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEpN1zipXqp3"
   },
   "source": [
    "# Opponent Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwQD5JJ638YT"
   },
   "outputs": [],
   "source": [
    "def idle_actor(frame_stack):\n",
    "    del frame_stack\n",
    "    return 0\n",
    "  \n",
    "def random_actor(frame_stack):\n",
    "    del frame_stack\n",
    "    return np.random.randint(NUM_ACTIONS)\n",
    "\n",
    "def no_bomb_random_actor(frame_stack):\n",
    "    del frame_stack\n",
    "    return np.random.randint(NUM_ACTIONS-1)\n",
    "  \n",
    "def model_actor(frame_stack, model):\n",
    "    obs = torch.from_numpy(np.array(frame_stack.get_observation()))\n",
    "    net_out = model(obs).detach().cpu().numpy()\n",
    "    action = np.argmax(net_out)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGESIdfBvewJ"
   },
   "source": [
    "# DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uB7UrkWABovb"
   },
   "outputs": [],
   "source": [
    "class DQNNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_stack, num_actions):\n",
    "        \"\"\" Create a DQN agent for Pommerman using Conv2d\n",
    "        Params\n",
    "        ======\n",
    "            num_stack (int): number of stacked images\n",
    "            num_actions (int): number of agent actions\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(num_stack, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_actions)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tensor(x, dtype=torch.float).to(device=device)\n",
    "        if len(x.size()) == 3:\n",
    "            x = x.unsqueeze(dim=0)\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQr5hKUUqdLG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_C4-J6QR63o"
   },
   "outputs": [],
   "source": [
    "dqn_save_path = \"gdrive/MyDrive/models\"\n",
    "dqn_load_path = \"game_1.pth\" # load up this checkpoint\n",
    "\n",
    "if os.path.exists(dqn_save_path) is False:\n",
    "    os.mkdir(dqn_save_path)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_path='', device=device):\n",
    "    \"\"\" Load up an existing checkpoint \n",
    "    Params\n",
    "    ======\n",
    "        checkpoint_path (String): path to a checkpoint .pth file\n",
    "        device (device): current device (cuda/cpu)\n",
    "    \"\"\"\n",
    "    dqn = DQNNet(num_stack=NUM_STACK, num_actions=NUM_ACTIONS).to(device)\n",
    "    dqn_target = DQNNet(num_stack=NUM_STACK, num_actions=NUM_ACTIONS).to(device)\n",
    "    optimizer = optim.Adam(dqn.parameters(), lr=LEARNING_RATE)\n",
    "    replay = ReplayBuffer(num_actions=NUM_ACTIONS, memory_len=BUFFER)\n",
    "    mse = torch.nn.MSELoss()\n",
    "    epoch = 0\n",
    "    timesteps = 0\n",
    "\n",
    "    if os.path.exists(os.path.join(dqn_save_path, dqn_load_path)):\n",
    "        print(f'Loading checkpoint {checkpoint_path}')\n",
    "        checkpoint_dict = torch.load(os.path.join(dqn_save_path, checkpoint_path), map_location=device)\n",
    "        timesteps = checkpoint_dict['timesteps']\n",
    "        dqn.load_state_dict(checkpoint_dict['model_params'])\n",
    "        dqn_target.load_state_dict(checkpoint_dict['model_params'])\n",
    "        optimizer.load_state_dict(checkpoint_dict['optimizer_state_dict'])\n",
    "        epoch = int(''.join(filter(str.isdigit, checkpoint_path)))\n",
    "        \n",
    "    return dqn, dqn_target, timesteps, epoch+1, optimizer, replay, mse\n",
    "\n",
    "\n",
    "def store_checkpoint(game_id, dqn_net, timesteps, optimizer):\n",
    "    \"\"\" Create a checkpoint by saving the network's dictionary, the number of timesteps and the optimizer's dictionary\n",
    "    Params\n",
    "    ======\n",
    "        game_id (int): number of episode\n",
    "        dqn_net (DQNNet): agent network\n",
    "        timesteps (int): number of passed timesteps (used for epsilon to avoid random exploration)\n",
    "        optimizer (state_dict): optimizer's state dictionary\n",
    "    \"\"\"\n",
    "    dqn_load_path = f'game_{game_id}.pth'\n",
    "    torch.save({'model_params': dqn_net.state_dict(),\n",
    "                'optimizer_state_dict': optimizer,\n",
    "                'timesteps': timesteps\n",
    "                },  os.path.join(dqn_save_path, dqn_load_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jF4z2A1n1FPo"
   },
   "outputs": [],
   "source": [
    "def soft_update(local_model, target_model, tau):\n",
    "    \"\"\" Soft update model parameters\n",
    "    θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "    Params\n",
    "    ======\n",
    "        local_model (DQNNet): weights will be copied from\n",
    "        target_model (DQNNet): weights will be copied to\n",
    "        tau (float): interpolation parameter \n",
    "    \"\"\"\n",
    "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "        target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqofB5qoUjF-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hkJbEP0W5FaJ"
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple(\"Transition\", [\"obs\", \"action\", \"reward\", \"next_obs\", \"done\"])\n",
    "import random\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, num_actions, memory_len = 10000):\n",
    "        self.memory_len = memory_len\n",
    "        self.transition = []\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    def add(self, obs, action, reward, next_obs, done):\n",
    "        \"\"\" add a transition to the buffer\n",
    "        Params\n",
    "        ======\n",
    "            obs (array): current board observation\n",
    "            action (int): best action in given observation\n",
    "            reward (int): -1 (lost), 0 (draw), 1 (won)\n",
    "            next_obs (array): next observation\n",
    "            done (Boolean): True (game finished), False (game still running)\n",
    "        \"\"\"\n",
    "        if self.length() > self.memory_len:\n",
    "            self.remove()\n",
    "        self.transition.append(Transition(obs, action, reward, next_obs, done))\n",
    "\n",
    "    def sample_batch(self, batch_size = 32):\n",
    "        \"\"\" get random batches from the buffer\n",
    "        Params\n",
    "        ======\n",
    "            batch_size (int): number of batches to include in the sample\n",
    "        \"\"\"\n",
    "        minibatch = random.sample(self.transition, batch_size)\n",
    "        obs_mb, a_, reward_mb, next_obs_mb, done_mb = map(np.array, zip(*minibatch))\n",
    "\n",
    "        mb_reward = torch.from_numpy(reward_mb).to(device=device, dtype=torch.float32)\n",
    "        mb_done = torch.from_numpy(done_mb.astype(int)).to(device=device)\n",
    "        a_ = a_.astype(int)\n",
    "        a_mb = np.zeros((a_.size, self.num_actions), dtype=np.float32)\n",
    "        a_mb[np.arange(a_.size), a_] = 1\n",
    "        mb_a = torch.from_numpy(a_mb).to(device=device)\n",
    "        return obs_mb, mb_a, mb_reward, next_obs_mb, mb_done\n",
    "\n",
    "    def length(self):\n",
    "        \"\"\" get length of buffer\n",
    "        \"\"\"\n",
    "        return len(self.transition)\n",
    "\n",
    "    def remove(self):\n",
    "        \"\"\" remove transistion from the buffer\n",
    "        \"\"\"\n",
    "        self.transition.pop(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bg87o5B-v5IP"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rl2A8v0pb1ff"
   },
   "outputs": [],
   "source": [
    "BUFFER = 1e6\n",
    "MINIBATCH = 128 \n",
    "DISCOUNT = 0.99 \n",
    "TAU = 0.01\n",
    "LEARNING_RATE = 1e-6\n",
    "UPDATE_RATE = 1000\n",
    "EPS_DECAY = 300000\n",
    "EPS_UB = 1.0\n",
    "EPS_LB = 0.02\n",
    "# set seed\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jDd7lJ_NKn2l"
   },
   "outputs": [],
   "source": [
    "network, target, timesteps, e, optimizer, replay, mse = load_checkpoint(dqn_load_path)\n",
    "\n",
    "reward_history = deque(maxlen=100)  # display last 100 rewards\n",
    "loss_history = deque(maxlen=100)    # display last 100 losses\n",
    "win_history = 0                     # count last 100 games outcomes\n",
    "\n",
    "# curriculum learning: train against different agents\n",
    "num_episodes =  [5000, 25000]\n",
    "actors = [no_bomb_random_actor, None]\n",
    "actor_curriculum = chain.from_iterable([[actor] * num_episodes for num_episodes, actor in zip(num_episodes, actors)])\n",
    "\n",
    "for actor in actor_curriculum: \n",
    "    env = PommerEnvWrapperFrameSkip2(num_stack=5, start_pos=random.randint(0,1), board='GraphicOVOCompact-v0', opponent_actor=actor) # create the 6x6 board \n",
    "    obs, _ = env.reset()\n",
    "    ret = 0\n",
    "    done = False\n",
    "\n",
    "    while not done: # play a game\n",
    "\n",
    "        # action selection according to epsilon\n",
    "        epsilon = max(EPS_LB, EPS_UB - timesteps/ EPS_DECAY)\n",
    "        if np.random.choice([0,1], p=[1-epsilon,epsilon]) == 1:\n",
    "            a = np.random.randint(low=0, high=NUM_ACTIONS, size=1)[0]\n",
    "        else:\n",
    "            net_out = network(obs).detach().cpu().numpy()\n",
    "            a = np.argmax(net_out)\n",
    "\n",
    "        # perform action\n",
    "        agent_step, _ = env.step(a)\n",
    "        next_obs, r, done, info = agent_step\n",
    "        ret += r\n",
    "        \n",
    "        # store transition in replay buffer\n",
    "        replay.add(obs, a, r, next_obs, done)\n",
    "        obs = next_obs\n",
    "        timesteps +=  1\n",
    "\n",
    "        # update policy using temporal difference\n",
    "        if replay.length() > MINIBATCH and replay.length() > UPDATE_RATE:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # sample a minibatch randomly\n",
    "            obs_mb, mb_a, mb_reward, next_obs_mb, mb_done = replay.sample_batch(MINIBATCH) \n",
    "\n",
    "            # compute predictions & targets\n",
    "            q = network(obs_mb).gather(1, torch.argmax(mb_a,1).unsqueeze(-1)).squeeze(-1)\n",
    "            q_t = target(next_obs_mb)\n",
    "            q_t[mb_done == 1] = 0.0\n",
    "            targets = mb_reward + (DISCOUNT * q_t.max(1)[0])\n",
    "            predictions = q    \n",
    "\n",
    "            # compute loss\n",
    "            loss = mse(predictions, targets) \n",
    "            loss.backward(retain_graph=False)\n",
    "            optimizer.step()\n",
    "            loss_history.append(loss.item())\n",
    "\n",
    "            # update policy\n",
    "            soft_update(network, target, TAU)\n",
    "\n",
    "    # game ended\n",
    "    print('Episode '+ str(e)+' Reward ' +str(ret))    \n",
    "    e += 1\n",
    "    reward_history.append(ret)\n",
    "\n",
    "    if r>0: \n",
    "        win_history += 1\n",
    "\n",
    "    if e % 100 == 0: \n",
    "        print('\\rStats for Episode {} (100 Games): \\tAverage Score: {:.2f}, Average Loss: {:.2f}, Win Probabilty: {:.2f} %'.format(e, np.mean(reward_history), np.mean(loss_history), win_history))\n",
    "        win_history = 0\n",
    "\n",
    "    if e % 500 == 0: # create a checkpoint (to continue training later) & export trained model as onnx (to evaluate performance)\n",
    "        store_checkpoint(e, network, timesteps, optimizer.state_dict()) \n",
    "        model_file = \"gdrive/MyDrive/submission.onnx\"\n",
    "        state_for_onnx = np.array(obs, dtype=np.float32)\n",
    "        torch.onnx.export(network,\n",
    "                          torch.from_numpy(state_for_onnx).float(), # example model input\n",
    "                          model_file, # file path\n",
    "                          export_params=True, # save trained parameters\n",
    "                          opset_version=10,\n",
    "                          do_constant_folding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlgtFCcpBove"
   },
   "source": [
    "## Export Agent in ONNX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03VfLlVJBove"
   },
   "outputs": [],
   "source": [
    "onnx_path = \"./submission.onnx\"\n",
    "state_for_onnx = np.array(obs, dtype=np.float32)\n",
    "torch.onnx.export(network,\n",
    "                  torch.from_numpy(state_for_onnx).float(), # example model input\n",
    "                  onnx_path, # file path\n",
    "                  export_params=True, # save trained parameters\n",
    "                  opset_version=10,\n",
    "                  do_constant_folding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uANu6eoMlMPj"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0m3PSVfEvwz"
   },
   "outputs": [],
   "source": [
    "# ignore prints to stdout of imports\n",
    "save_stdout = sys.stdout\n",
    "sys.stdout = open('trash', 'w')\n",
    "from graphic_pomme_env import graphic_pomme_env\n",
    "sys.stdout = save_stdout\n",
    "from graphic_pomme_env.wrappers import PommerEnvWrapperFrameSkip2\n",
    "\n",
    "# Seed random number generators\n",
    "if os.path.exists(\"seed.rnd\"):\n",
    "    with open(\"seed.rnd\", \"r\") as f:\n",
    "        seed = int(f.readline().strip())\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "else:\n",
    "    seed = None\n",
    "\n",
    "N_EPISODES = 50\n",
    "RAND_PERF = 0.5\n",
    "\n",
    "model_file = \"submission.onnx\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Network\n",
    "net = ConvertModel(onnx.load(model_file), experimental=True)\n",
    "net = net.to(device)\n",
    "net.eval()\n",
    "\n",
    "win_count = 0.0\n",
    "env = PommerEnvWrapperFrameSkip2(num_stack=5, start_pos=0)\n",
    "\n",
    "for i in range(N_EPISODES):\n",
    "    if seed is not None:\n",
    "        seed = np.random.randint(1e7)\n",
    "    \n",
    "    done = False\n",
    "    obs, opponent_obs = env.reset()\n",
    "    while not done:\n",
    "        obs = torch.from_numpy(np.array(obs)).to(device)\n",
    "        net_out = net(obs).detach().cpu().numpy()\n",
    "        action = np.argmax(net_out)\n",
    "\n",
    "        agent_step, opponent_step = env.step(action)\n",
    "        obs, r, done, info = agent_step\n",
    "        \n",
    "        if done and r > 0:\n",
    "            win_count += 1.0\n",
    "            \n",
    "win_count /= N_EPISODES\n",
    "win_count = (win_count - RAND_PERF) / (1.0 - RAND_PERF)\n",
    "            \n",
    "print(win_count)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "code.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
